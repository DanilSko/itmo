{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как использовать этот код \n",
    "### (можно пропустить эту часть, если вы знаете, что такой Jupyter и ipynb-тетрадки -- в таком случае переходите сразу к \"Начало работы\")\n",
    "Перед вами тетрадка Jupyter Notebook. Это одна из популярных сред для написания и __демонстрации__ кода на Python (и не только). Jupyter запускает питоновский код прямо в браузере (но локально, т.е. код исполняет ваш компьютер, в отличие от, например Google Colab). В Jupyter код можно запускать не целиком, а по ячейкам. Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ячейка с кодом\n",
    "text = 'Мы с Даней сегодня позапускаем морфоанализаторы'\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# еще одна ячейка с кодом. \n",
    "# Пространство имен общее — переменная text была заполнена в предыдущей ячейке\n",
    "print (text.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, ячейки бывают текстовыми — их можно форматировать с помощью формата разметки markdown (.md). Текст, который вы сейчас читаете, написан как раз в таком формате. Файлы Jupyter Notebook имеют расширение .ipynb и автоматически рендерятся гитхабом. Например, эта же __[тетрадка](https://github.com/DanilSko/itmo/blob/master/ITMO_practice/BasicProcessing/Basic%20text%20processing%20Danya%20ITMO.ipynb)__  у меня на гитхабе.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Окей, а как я могу запустить код в таком .ipynb?\n",
    "\n",
    "Есть разные варианты: \n",
    "\n",
    "### Через __[Google Colab](https://colab.research.google.com)__. \n",
    "\n",
    "Google Colab — это гугловский инструмент для написания кода в браузере и запуска прямо на серверах Google (с возможностью бесплатно использовать их вычислительные мощности, в т.ч. графические процессор  — GPU).  Google Colab — родственник Jupyter, они очень похожи. Разница в том, что Jupyter работает локально и использует ваш собственный питон, а Google Colab — это облачный сервис, похожий на Google Docs: вы можете делиться тетрадками и т.д. В Colab можно открыть эту тетрадку, указав ссылку на нее: \n",
    "\n",
    "![Colab](pics/github2colab.png) \n",
    "\n",
    "Просто загрузить .ipynb-файл в Colab тоже можно. \n",
    "\n",
    "### Скачать себе .ipynb и открыть в Jupyter\n",
    "\n",
    "Скачать .ipynb с гитхаба: \n",
    "\n",
    "![Raw](pics/raw_download.png) \n",
    "\n",
    "Поставить Jupyter Notebook по инструкции __[отсюда](https://jupyter.readthedocs.io/en/latest/install.html)__. Открыть терминал (в Windows — командную строку).пойти в папку, внутри которой лежит скачанный .ipynb, написать там jupyter notebook. Должно появиться что-то такое:\n",
    "\n",
    "![Colab](pics/github2colab.png)\n",
    "\n",
    "Из Jupyter можно выгрузить код в виде файла .py\n",
    "\n",
    "### Скачать .ipynb и открыть в PyCharm\n",
    "Если вы любите популярную у питонистов IDE PyCharm  -- она умеет открывать ipynb. Но для этого все равно нужен установеленный Jupyter. Поэтому надо проделать все то же, что в предыд.пункте, а потом открыть файл в PyCharm.\n",
    "\n",
    "### (хак) Скачать файл .py из Google Colab или Jupyter и открыть в PyCharm или любой IDE\n",
    "Из Colab после создания копии вы можете выгрузить не весь .ipynb, а только код в .py. Этот код уже можно запускать где угодно, хоть из командной строки (терминала) вашего компьютера. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Начало работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим, что рядом в папке лежит файл с Преступлением и наказанием.\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## У тех, кто смотрит в колабе, его, естественно, не будет. Поэтому вот стандартный код для загрузки в колаб\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __[Ссылка на текстовый файл с Преступлением и наказанием](https://github.com/DanilSko/itmo/blob/master/ITMO_practice/BasicProcessing/Dostoevsky_PrestuplenieINakazanie.txt)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# откроем файл в питоне\n",
    "path_to_file = 'Dostoevsky_PrestuplenieINakazanie.txt'\n",
    "prest_i_nak = open (path_to_file, 'r')\n",
    "prest_i_nak_kak_stroka = prest_i_nak.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## попробуйте вывести тут длину строки с помощью функции len; надо написать len (prest_i_nak_kak_stroka) -- так вы передадите в функцию len ваш текст, считанный в сторку "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## попробуйте получить срез из первых 100 символов вашей строки с помощью срезов списка [:]\n",
    "## т.е. напишите ниже вот это: prest_i_nak_kak_stroka [:100]\n",
    "## и нажмите выполнить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сегментация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Простейший способ сегментации строки на токены (как бы на слова, но тупее грубее) — питоновский встроенный метод .split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prest_i_nak_kak_spisok_slov = prest_i_nak_kak_stroka.split(' ') ## здесь в качестве аргумента можно воткнуть разделитель; по умолчанию это любое количество пробелов\n",
    "print ('Примерное количество слов в \"Преступлении и наказании\":', len (prest_i_nak_kak_spisok_slov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prest_i_nak_kak_spisok_slov[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### попробуем получить список уникальных слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prest_i_nak_kak_mnojestvo_slov = set (prest_i_nak_kak_spisok_slov)\n",
    "len (prest_i_nak_kak_mnojestvo_slov)\n",
    "prest_i_nak_kak_spisok_unikalnyh_slov = list (prest_i_nak_kak_mnojestvo_slov)\n",
    "(prest_i_nak_kak_spisok_unikalnyh_slov).sort()\n",
    "prest_i_nak_kak_spisok_unikalnyh_slov [:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Более умный способ: сегментируем текст регекспом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## питоновский модуль для регекспов\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prest_i_nak_spisok_re =  re.split ('( +|[.,!? –]|\\n)',prest_i_nak_kak_stroka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prest_i_nak_spisok_re [:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Еще более умный способ: сегментируем текст готовым токенизатором — возьмем его из прекрасной библиотеки для обработки языка NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если у вас еще нет nltk, установите его:\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "prest_i_nak_nltk_tokenized = word_tokenize (prest_i_nak_kak_stroka)\n",
    "prest_i_nak_nltk_tokenized_sample = prest_i_nak_nltk_tokenized [:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prest_i_nak_nltk_tokenized_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стемминг, лемматизация, морфологический анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В том же NLTK есть готовая реализация стеммера для русского языка. Давайте потестируем ее!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## этот стеммер не умеет сам токенизировать -- он работает только с отдельными словами. \n",
    "## Поэтому придется скармливать ему наш список по одному: \n",
    "prest_i_nak_stemmed = []\n",
    "for word in prest_i_nak_nltk_tokenized_sample:\n",
    "    prest_i_nak_stemmed.append (stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prest_i_nak_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyStem\n",
    "\n",
    "__[Mystem](https://tech.yandex.ru/mystem/)__ - это свободно распространяемый морфологический анализатор для русского языка с закрытым исходным кодом. То есть мы можем его бесплатно скачать с сайта и пользоваться им, но не можем посмотреть, что у него внутри и как оно работает.\n",
    "\n",
    "Mystem был придуман одним из создателей Яндекса Ильёй Сегаловичем. Некоторый потомок Mystem'а до сих пор работает внутри большого поисковика Яндекса, анализируя слова при поиске.\n",
    "\n",
    "MyStem значит my stemmer. Стемминг -- это разбиение формы на основу и флексию. Программы-стеммеры умеют превращать фразу 'Маша поехала за грибами' в 'Маш поехал за гриб'.  \n",
    "\n",
    "Но на самом деле MyStem не стеммер, а полноценный морфологический анализатор. Он может гораздо больше: устанавливать словарную форму слова, определять часть речи и грамматические характеристики (падеж, число, время, род...). В последних версиях Mystem умеет и выбирать из нескольких возможных грамматических разборов один, наиболее верный.\n",
    "\n",
    "У Mystem нет графического оконного интерфейса, запустить его можно только из командной строки. Зато есть обертка для Python — pymystem3.\n",
    "\n",
    "__[Документация к MyStem](https://tech.yandex.ru/mystem/doc/index-docpage/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# эта клетка нужна вам, если у вас не установлен модуль pymystem3. \n",
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## импортируем непосредственно класс \"анализатор MyStem\" из pymystem3\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "help (Mystem()) ## всегда полезно почитать хелпы!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "moi_analizator = Mystem() ## создаем анализатор\n",
    "test = 'Даня тестирует машинную морфологию' ## создаем тестовую строку\n",
    "lemmatized = moi_analizator.lemmatize(test) ## лемматизируем строку с помощью mystem \n",
    "print (lemmatized) ## напечатаем лемматизированную строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prest_i_nak_by_mystem = moi_analizator.analyze(prest_i_nak_kak_stroka[:100]) # произведем морфологический анализ с помощью mystem \n",
    "print  (prest_i_nak_kak_by_mystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
